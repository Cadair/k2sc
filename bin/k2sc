#!/usr/bin/env python
from __future__ import print_function, division
import os
import sys
import errno
import warnings
import logging

import numpy as np
import scipy as sp
import pyfits as pf
import matplotlib.pyplot as pl

try:
    from mpi4py import MPI
    comm = MPI.COMM_WORLD
    mpi_rank = comm.Get_rank()
    mpi_size = comm.Get_size()
    with_mpi = True
except ImportError:
    mpi_rank = 0
    mpi_size = 1
    with_mpi = False

from copy import copy   
from collections import namedtuple
from matplotlib.backends.backend_pdf import PdfPages
from numpy import any, array, ones_like, fromstring, tile, median, isfinite, nanmean
from numpy.random import normal

from time import time, sleep
from datetime import datetime
from os.path import join, exists, abspath, basename
from argparse import ArgumentParser

from k2sc.core import *
from k2sc.detrender import Detrender
from k2sc.kernels import kernels, BasicKernel, QuasiPeriodicKernel
from k2sc.k2io import select_reader, FITSWriter, SPLOXReader
from k2sc.cdpp import cdpp
from k2sc.de import DiffEvol
from k2sc.ls import fasper

warnings.resetwarnings()
warnings.filterwarnings('ignore', category=UserWarning, append=True)
warnings.filterwarnings('ignore', category=DeprecationWarning, append=True)

##TODO: Copy the FITS E0 header if a fits file is available
##TODO: Define the covariance matrix splits for every campaign

mpi_root = 0
logging.basicConfig(level=logging.DEBUG, format='%(levelname)s %(name)s: %(message)s')

def nanmedian(a):
    return np.median(a[isfinite(a)])


def detrend(dataset):
    ## Setup the logger
    ## ----------------
    logger  = logging.getLogger('Worker %i'%mpi_rank)
    logfile = open('{:s}.{:03d}'.format(args.logfile, mpi_rank), mode='w')
    fh = logging.StreamHandler(logfile)
    fh.setFormatter(logging.Formatter('%(levelname)s %(name)s: %(message)s'))
    fh.setLevel(logging.DEBUG)
    logger.addHandler(fh)
    fpath_out = join(args.save_dir, reader.fn_out_template.format(dataset.epic))
    logger.name = 'Worker {:d} <{:d}>'.format(mpi_rank, dataset.epic)

    ## Initialise utility variables
    ## ----------------------------
    ds = dataset
    info = logger.info

    ## Periodic signal masking
    ## -----------------------
    if args.p_mask_center and args.p_mask_period and args.p_mask_duration:
        ds.mask_periodic_signal(args.p_mask_center, args.p_mask_period, args.p_mask_duration)

    ## Initial outlier and period detection
    ## ------------------------------------
    ## We carry out an initial outlier and period detection using
    ## a default GP hyperparameter vector based on campaign 4 fits
    ## done using (almost) noninformative priors.
    masks = []
    for iset in range(ds.nsets):
        flux = ds.fluxes[iset]
        inputs = np.transpose([ds.time,ds.x,ds.y])
        detrender = Detrender(flux, inputs, mask=isfinite(flux),
                            splits=splits, kernel=BasicKernel(),
                            tr_nrandom=args.tr_nrandom,
                            tr_nblocks=args.tr_nblocks, tr_bspan=args.tr_bspan)
    
        ttrend,ptrend = detrender.predict(detrender.kernel.pv0+1e-5, components=True)
        cflux = flux-ptrend+median(ptrend)-ttrend+median(ttrend)
        cflux /= nanmedian(cflux)
        mad = nanmedian(abs(cflux-nanmean(cflux)))
        sigma = 1.4826*mad

        fmask = isfinite(flux)
        omask = ones_like(fmask)
        omask[fmask] = (cflux[fmask] < 1+5*sigma) & (cflux[fmask] > 1-5*sigma)

        masks.append(fmask)
        if (~omask).sum() / omask.size < 0.25:
            masks[iset] &= omask

        ## Lomb-Scargle period search
        ## --------------------------
        info('Starting Lomb-Scargle period search')
        nflux = flux - ptrend + nanmedian(ptrend)
        for split in detrender.gp.splits:
            i = np.argmin(abs(ds.time - split))
            nflux[i:] += nanmedian(nflux[i-10:i]) - nanmedian(nflux[i:i+10])
        
        freq,power,nout,jmax,prob = fasper(ds.time[masks[iset]], nflux[masks[iset]], 6, 0.5)
        period = 1./freq

        m = (period > args.ls_min_period) & (period < args.ls_max_period) 
        period, power = period[m], power[m]
        j = np.argmax(power)
        expy = np.exp(-power[j])
        effm = 2.*nout/6
        fap  = expy*effm
        if fap > 0.01: 
            fap = 1.0-(1.0-expy)**effm

        if fap < 0.01:
            ds.is_periodic = True
            ds.ls_power = fap
            ds.ls_period = period[j]
        
    ## Kernel selection
    ## ----------------
    if args.kernel:
        info('Overriding automatic kernel selection, using %s kernel as given in the command line', args.kernel)
        if 'periodic' in args.kernel and not args.kernel_period:
            logger.critical('Need to give period (--kernel-period) if overriding automatic kernel detection with a periodic kernel. Quitting.')
            exit(1)
        kernel = kernels[args.kernel](period=args.kernel_period)
    else:
        if ds.is_periodic:
            info('Found periodicity p = {:7.2f} (power {:7.2f} > {:7.2f}), using a quasiperiodic kernel'.format(ds.ls_period, ds.ls_power, args.ls_min_power))
            kernel = QuasiPeriodicKernel(period=ds.ls_period)
        else:
            info('No strong periodicity found, using a basic kernel')
            kernel = BasicKernel()

    ## Detrending
    ## ----------
    Result = namedtuple('SCResult', 'detrender pv tr_time tr_position cdpp_r cdpp_t cdpp_c warn')
    results = []
    for iset in range(ds.nsets):
        if ds.nsets > 1:
            logger.name = 'Worker {:d} <{:d}-{:d}>'.format(mpi_rank, dataset.epic, iset+1)
        np.random.seed(args.seed)
        tstart = time()
        inputs = np.transpose([ds.time,ds.x,ds.y])
        detrender = Detrender(ds.fluxes[iset], inputs, mask=masks[iset],
                              splits=splits, kernel=kernel, tr_nrandom=args.tr_nrandom,
                              tr_nblocks=args.tr_nblocks, tr_bspan=args.tr_bspan)
        de = DiffEvol(detrender.neglnposterior, kernel.bounds, args.de_npop)

        if isinstance(kernel, QuasiPeriodicKernel):
            de._population[:,2] = np.clip(normal(kernel.period, 0.1*kernel.period, size=de.n_pop),
                                          args.ls_min_period, args.ls_max_period)

        info('Starting global hyperparameter optimisation using DE')
        tstart_de = time()
        for i,r in enumerate(de(args.de_niter)):
            info('DE iteration %3i -ln(L) %4.1f', i, de.minimum_value)
            tcur_de = time()
            if ((de._fitness.ptp() < 3) or (tcur_de - tstart_de > args.de_max_time)) and (i>2):
                break
        info('DE finished in %i seconds', tcur_de-tstart_de)
        info('DE minimum found at: %s', np.array_str(de.minimum_location, precision=3, max_line_width=250))
        info('DE -ln(L) %4.1f', de.minimum_value)

        info('Starting local hyperparameter optimisation')
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', category=RuntimeWarning, append=True)
                pv, warn = detrender.train(de.minimum_location)
        except ValueError as e:
            logger.error('Local optimiser failed, %s', e)
            logger.error('Skipping the file')
            return
        info('Local minimum found at: %s', np.array_str(pv, precision=3))

        info('Masking outliers')
        flux = detrender.data.unmasked_flux
        ttrend,ptrend = detrender.predict(pv, components=True)
        cflux = flux-ptrend+median(ptrend)-ttrend+median(ttrend)
        cflux /= nanmedian(cflux)
        mad = nanmedian(abs(cflux-nanmean(cflux)))
        sigma = 1.4826*mad

        minf  = isfinite(cflux)
        mhigh = np.zeros_like(minf)
        mlow  = np.zeros_like(minf)
        mhigh[minf] = cflux[minf] > 1+5*sigma
        mlow[minf]  = cflux[minf] < 1-5*sigma
        mask = minf & (~mlow) & (~mhigh)

        ds.mflags[iset][~minf] |= M_NOTFINITE
        ds.mflags[iset][mhigh] |= M_OUTLIER_U
        ds.mflags[iset][mlow]  |= M_OUTLIER_D
        
        info('Computing time and position trends')
        tr_time,tr_position = detrender.predict(pv, components=True)
        cdpp_r = cdpp(detrender.data.masked_time,
                      detrender.data.masked_flux)
        cdpp_t = cdpp(detrender.data.unmasked_time,
                      detrender.data.unmasked_flux-tr_position + np.median(tr_position),
                      exclude=~detrender.data.mask)
        cdpp_c = cdpp(detrender.data.unmasked_time,
                      detrender.data.unmasked_flux-tr_time-tr_position+median(tr_time)+median(tr_position),
                      exclude=~detrender.data.mask)
        results.append(Result(detrender, pv, tr_time, tr_position, cdpp_r, cdpp_t, cdpp_c, warn))
        info('CDPP - raw - %6.3f', cdpp_r)
        info('CDPP - position component removed - %6.3f', cdpp_t)
        info('CDPP - full reduction - %6.3f', cdpp_c)
        info('Detrending time %6.3f', time()-tstart)

    FITSWriter.write(fpath_out, splits, ds, results)
    info('Finished')
    fh.flush()
    logger.removeHandler(fh)
    fh.close()
    logfile.close()


if __name__ == '__main__':
    ap = ArgumentParser(description='K2SC: K2 systematics correction using Gaussian processes')
    gts = ap.add_argument_group('Training set options')
    gps = ap.add_argument_group('Period search', description='Options to control the initial Lomb-Scargle period search')
    gfd = ap.add_argument_group('Flare detection', description='Options to control the detection and masking of flares')
    god = ap.add_argument_group('Outlier detection')
    gde = ap.add_argument_group('Global optimisation', description='Options to control the global hyperparameter optimisation')
    ap.add_argument('files', metavar = 'F', type=str, nargs='*', help='Input light curve file name.')
    ap.add_argument('-c', '--campaign', metavar='C', type=int, help='Campaign number')
    ap.add_argument('--splits', default=None, type=lambda s:fromstring(s.strip('[]'), sep=','), help='List of time values for kernel splits')
    ap.add_argument('--quiet', action='store_true', default=False, help='suppress messages')
    ap.add_argument('--save-dir', default='.', help='The directory to save the output file in')
    ap.add_argument('--start-i', default=0, type=int)
    ap.add_argument('--end-i', default=None, type=int)
    ap.add_argument('--seed', default=0, type=int)
    ap.add_argument('--logfile', default='', type=str)
    ap.add_argument('--flux-type', default='sap', type=str)
    ap.add_argument('--kernel', choices=kernels.keys(), default=None)
    ap.add_argument('--kernel-period', type=float, default=None)
    ap.add_argument('--p-mask-center', type=float, default=None)
    ap.add_argument('--p-mask-period', type=float, default=None)
    ap.add_argument('--p-mask-duration', type=float, default=None)
    gts.add_argument('--tr-nrandom', default=300, type=int, help='Number of random samples')
    gts.add_argument('--tr-nblocks', default=6, type=int, help='Number of sample blocks')
    gts.add_argument('--tr-bspan', default=50, type=int, help='Span of a single block')
    gde.add_argument('--de-npop', default=100, type=int, help='Size of the differential evolution parameter vector population')
    gde.add_argument('--de-niter', default=150, type=int, help='Number of differential evolution iterations')
    gde.add_argument('--de-max-time', default=300, type=float, help='Maximum time used for differential evolution')
    gps.add_argument('--ls-min-power', default=100, type=float, help='Lomb-Scargle power treshold for the periodic kernel')
    gps.add_argument('--ls-min-period', default=0.20, type=float, help='Minimum period to search for')
    gps.add_argument('--ls-max-period', default=20, type=float, help='Maximum period to search for')
    gfd.add_argument('--flare-sigma', default=5, type=float)
    gfd.add_argument('--flare-erosion', default=5, type=int)
    god.add_argument('--outlier-sigma', default=5, type=float)
    god.add_argument('--outlier-mwidth', default=25, type=int)
    args = ap.parse_args()

    csplits = {4: [2240,2273]}

    ## Logging
    ##
    if mpi_rank == 0:
        logger = logging.getLogger('Master')
        if args.logfile:
            logfile = open(args.logfile, mode='w')
            fh = logging.StreamHandler(logfile)
            fh.setFormatter(logging.Formatter('%(levelname)s %(name)s: %(message)s'))
            fh.setLevel(logging.DEBUG)
            logger.addHandler(fh)

    if args.splits is None:
        splits = csplits[args.campaign]
    else:
        splits = args.splits

    if not exists(args.save_dir):
        logger.error("Error: the save directory {:s} doesn't exists".format(args.save_dir), file=sys.stderr)
        exit(errno.ENOENT)

    ## Select data reader
    ## NOTE: We don't allow mixed input types per run
    reader = select_reader(args.files[0])

    if reader is None:
        logger.critical("Unrecognized input file type for file {:s}".format(args.files[0]))
        exit()
                
    ## Check if we're dealing with a single input file with many stars
    if len(args.files) == 1 and issubclass(reader, SPLOXReader):
        singlefile = True
        infile = args.files[0]
        sid = args.start_i
        all_items = range(args.start_i, min(args.end_i or 1e8, reader.nstars(args.files[0])))
    else:
        singlefile = False
        infile = args.files[args.start_i]
        sid = 0
        all_items = args.files[args.start_i:args.end_i]
    items = copy(all_items)
    n_items = len(items)

    if mpi_rank == 0:
        if (not with_mpi) or (mpi_size==1):
            logger.info("Detrending {:d} light curves without MPI".format(n_items))
        else:
            logger.info("Detrending {:d} light curves using {:d} worker nodes".format(n_items, mpi_size-1))
        logger.info('')
        logger.info('Saving the results to %s', args.save_dir)
        logger.info('')
        logger.info('Differential evolution parameters')
        logger.info('  Population size: {:3d}'.format(args.de_npop))
        logger.info('  Number of iterations: {:3d}'.format(args.de_niter))
        logger.info('  Maximum DE time: {:6.2f} seconds'.format(args.de_max_time))
        logger.info('')

    ## Without MPI or running with a single node
    ## =========================================
    if (not with_mpi) or (mpi_size==1):
        for item in items:
            if singlefile:
                sid = item
            else:
                infile = item
                if not exists(infile):
                    logger.warning("The input file {:s} doesn't exists, skipping the file".format(infile))
                    continue
            dataset = reader.read(infile, sid=sid, type=args.flux_type)
            detrend(dataset)
                                

    ## With MPI
    ## ========
    else:
        ## Master node
        ## -----------
        if mpi_rank == 0:
            free_workers = range(1,mpi_size)
            active_workers = []
            n_finished_items = 0

            while items or active_workers:
                ## Send an item
                while items and free_workers:
                    w = free_workers.pop()

                    item = items.pop()

                    if singlefile:
                        sid = item
                        logger.info("Processing star %i", sid)
                    else:
                        infile = item
                        logger.info("Processing file %s",abspath(infile))
                        if not exists(infile):
                            logger.warning("The input file {:s} doesn't exists, skipping the file".format(infile))
                            continue
                        
                    dataset = reader.read(infile, sid=sid, type=args.flux_type)
                    comm.send(dataset, dest=w, tag=0)
                    active_workers.append(w)

                ## Receive the results
                for w in active_workers:
                    if comm.Iprobe(w, 2):
                        res = comm.recv(source=w, tag=2)
                        free_workers.append(w)
                        active_workers.remove(w)
                        n_finished_items += 1

                        if args.logfile:
                            logfile_w = open('{:s}.{:03d}'.format(args.logfile, w), 'r')
                            logfile.write(logfile_w.read())
                            logfile_w.close()
                        logger.info("Finished {:3d} of {:3d} light curves".format(n_finished_items,n_items))

            for w in free_workers:
                comm.send(-1, dest=w, tag=0)

        ## Worker node
        ## -----------
        else:
            while True:
                dataset = comm.recv(source=mpi_root, tag=0)
                if infile == -1:
                    break

                detrend(dataset)
                comm.send(dataset.epic, dest=mpi_root, tag=2)    


        
